{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Unit 9</h1> </center>\n",
    "<center> <h1>Case Study 3: Credit Score Prediction with Random Forests</h1></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 â€“ Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "- The Credit Score problem.  \n",
    "\n",
    "- Logistic regression.  \n",
    "\n",
    "- Decision Trees.  \n",
    "\n",
    "- The wisdom of the crowds.  \n",
    "\n",
    "- Bagged Decision Trees.  \n",
    "\n",
    "- Random Forest.  \n",
    "\n",
    "- Gradient boosting.  \n",
    "\n",
    "- Spark ML demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The credit score problem\n",
    "- Predict whether someone will be in financial distress.  \n",
    "\n",
    "- Data from Kaggle: https://www.kaggle.com/c/GiveMeSomeCredit  \n",
    "\n",
    "<div class=\"blockquote2\">\n",
    "Credit scoring algorithms, which **make a guess at the probability of default**, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by **predicting the probability that somebody will experience financial distress in the next two years**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The credit score problem (2)\n",
    "- Predict whether someone will be in financial distress.  \n",
    "\n",
    "- Data from Kaggle: https://www.kaggle.com/c/GiveMeSomeCredit  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf1.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The credit score problem (3) \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf2.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A problem with logistic regression\n",
    "- Remember the model we are trying to learn:  \n",
    "\n",
    "$$p(y \\mid X) = \\frac{1}{1 + \\exp(-(\\theta_0 + \\sum_{j>0}{x_j\\theta_j}))}$$  \n",
    "\n",
    "- How is the boundary between classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The XOR problem\n",
    "- Two features, two classes, and 4 data points.  \n",
    "\n",
    "- Can we use logistic regression to fit the exclusive-or function?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf3.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The XOR problem (2)\n",
    "- This problem created a Neural Network winter for 20 years.  \n",
    "\n",
    "- How to solve it?  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf3.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The XOR problem (3)\n",
    "- Non-linearity, more complex functions!  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf4.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-5\">\n",
    "      <ul>\n",
    "        <li>Decision tree recursively partitions the feature space.</li>\n",
    "        <br>\n",
    "        <li>The decisions are typically simple.</li>\n",
    "        <br>\n",
    "        <li>Because of the recursive partitioning, the decision can be represented as a tree.</li>\n",
    "      </ul>        \n",
    "    </div>\n",
    "    <div class=\"col-7\">\n",
    "      <ul>\n",
    "        <center><img src=\"./images/unit-09/unit-09-0_cscsrf5.png\" width=\"100%\" align=\"center\"></center>\n",
    "      </ul>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (2)\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-5\">\n",
    "      <ul>\n",
    "          <center><img src=\"./images/unit-09/unit-09-0_cscsrf6.png\" width=\"80%\" align=\"center\"></center>\n",
    "      </ul>        \n",
    "    </div>\n",
    "    <div class=\"col-7\">\n",
    "      <ul>\n",
    "        <li>Propose a decision tree to fit this dataset:</li>\n",
    "      </ul>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (3)\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-5\">\n",
    "      <ul>\n",
    "        <center><img src=\"./images/unit-09/unit-09-0_cscsrf6.png\" width=\"80%\" align=\"center\"></center>\n",
    "      </ul>        \n",
    "    </div>\n",
    "    <div class=\"col-7\">\n",
    "      <ul>\n",
    "        <center><img src=\"./images/unit-09/unit-09-0_cscsrf7.png\" width=\"80%\" align=\"center\"></center>\n",
    "      </ul>\n",
    "    </div>\n",
    "  </div>      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (4)\n",
    "- Recursively partition the space into M regions:  \n",
    "\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf8.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (5)\n",
    "- Recursively partition the space into M regions:  \n",
    "\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf9.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (6)\n",
    "- Recursively partition the space into M regions:  \n",
    "\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf10.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (7)\n",
    "- Recursively partition the space into M regions:  \n",
    "\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf11.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees (7)\n",
    "- For regression, predict the average of a region and evaluate on squared error:  \n",
    "\n",
    "$$\\sum_{m=1}^{M}\\sum_{i \\in R_m} {\\left(y_i - \\hat y_{R_m}\\right)^2}$$\n",
    "\n",
    "- For classification, predict the majority of a region and evaluate on distribution of predictions:  \n",
    "<br>\n",
    "  **Gini index** $\\qquad\\; G_m = \\sum_{k=1}^{K} {\\hat p_{mk}(1 - \\hat p_{mk}})$  \n",
    "<br>\n",
    "  **Cross entropy** $\\quad D_m = - \\sum_{k=1}^{K} {\\hat p_{mk}\\log(\\hat p_{mk}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision tree: Optimization\n",
    "- In general, it is intractable to search the entire space of possible regions.  \n",
    "\n",
    "- In practice, build tree from top down:\n",
    "  1. Select the feature $j$ and cut point that reduces the loss function the most.\n",
    "  2. Keep applying this rule recursively until a minimum of points in a leaf.  \n",
    "  \n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf12.png\" width=\"30%\" align=\"center\"></center>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision tree: Optimization (2)\n",
    "- Full tree usually overfits badly, we need to prune.  \n",
    "\n",
    "- Pruning can be done with regularization:  \n",
    "\n",
    "  Start from full tree $T_0$ and for each of a sequence of regularization parameters $\\alpha=\\{0,0.01, 0.02, \\ldots\\}$ find the best cost function by removing regions:  \n",
    "\n",
    "$$\\sum_{m=1}^{M}\\sum_{i \\in R_m} {L\\left(y_i,\\hat f(x_i)\\right)} + \\alpha M$$  \n",
    "<br>\n",
    "$\\qquad \\alpha$ penalizes de total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with decision trees?\n",
    "- How many ways you have to fit a dataset?  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf12.png\" width=\"40%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problems with decision trees? (2)\n",
    "- How many ways you have to fit a dataset?: too many! In fact, infinitely many!  \n",
    "\n",
    "- In one word: Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros from ISRL book\n",
    "- Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!  \n",
    "\n",
    "- Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.  \n",
    "\n",
    "- Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).  \n",
    "- Trees can easily handle qualitative predictors without the need to create dummy variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cons from ISRL book\n",
    "- Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.  \n",
    "\n",
    "- Additionally, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "- Problem with decision trees is that they have high variance.  \n",
    "\n",
    "- One solution is to average the prediction of several trees fit in different datasets.  \n",
    "\n",
    "- Why would this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <li>Imagine you ask $B$ individuals the opinion about the height of someone.</li>\n",
    "        <br>\n",
    "        <li>Suppose that each of them have the same variance $v$</li>\n",
    "        <br>\n",
    "        <li>What would happen to the variance of the average opinion?</li>\n",
    "      </ul>        \n",
    "    </div>\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <center><img src=\"./images/unit-09/unit-09-0_cscsrf13.png\" width=\"100%\" align=\"center\"></center>\n",
    "      </ul>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Central Limit Theorem\n",
    "**Lindeberg-LÃ©vy CLT**. Suppose $\\{X1, X2, \\ldots\\}$ is a sequence of i.i.d. random variables with $E[X_i] = \\mu$ and $Var[Xi] = \\sigma^2 < \\infty$. Then as $n$ approaches infinity, the random variables $\\sqrt {n}\\left(S_{n}-\\mu \\right)$ converge in distribution to a normal $N\\left(0,\\sigma ^{2}\\right)$:\n",
    "\n",
    "$$\\sqrt {n}\\left(\\left(\\frac{1}{n} \\sum_{i=1}^{n} {X_i} \\right) - \\mu \\right)\\ {\\xrightarrow {d}}\\ N\\left(0,\\sigma ^{2}\\right)$$  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf14.png\" width=\"25%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wisdom of the crowds (cont.)\n",
    "- If opinions about height are normally distributed with variance $v$.  \n",
    "\n",
    "- The average opinion of $B$ will have variance $v/B$.  \n",
    "\n",
    "- Example, with no bias and variance of 10 inches per opinion:  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf15.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging (cont.)\n",
    "- In bagging, we simulate training datasets by bootstrapping training data (sample with replacement.)  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf16.png\" width=\"60%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree Bagging\n",
    "- We can therefore fit several decision trees to bootstrapped dataset.  \n",
    "\n",
    "- We average the predictions of the models and therefore reduce variance.  \n",
    "<br>\n",
    "  **Regression** $\\qquad \\hat{f}_{\\text{bagging}}(X) = \\frac{1}{B}\\sum_{b=1}^{B}{\\hat{f}^{*b}(X)}$  \n",
    "<br>\n",
    "  **Classification** $\\quad \\hat{f}_{\\text{bagging}}(X) = \\text{majority}\\{\\hat{f}^{*b}(X) \\mid b \\in \\{1, \\ldots , B\\}\\}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree Bagging (2)\n",
    "- In bagging, we can get estimate test error for free.  \n",
    "\n",
    "- Each data point will have a 1/3 chance of not being sampled!  \n",
    "\n",
    "- We can use those out of bag (OOB) data points for testing performance.  \n",
    "\n",
    "- Derivation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limitations of bagging\n",
    "- Because we use the same training data over and over, trees become correlated.  \n",
    "\n",
    "- What happens to correlation in wisdom of the crowds?  \n",
    "\n",
    "- If I take the average opinion as my prediction, how should I predict the results of the election?:\n",
    "  1. I take each of you outside and I ask you who will win the election.\n",
    "  2. I ask each of you in front of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Correlations ruin bagging\n",
    "- Variance of estimator with correlations:  \n",
    "\n",
    "$$var(f) = \\left(\\frac{1 - \\rho}{B} + \\rho \\right) \\sigma^2$$  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf17.png\" width=\"45%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random forest\n",
    "- Random forest tries to remove correlation by randomly sampling $m$ predictions at each split.  \n",
    "\n",
    "- Therefore, each tree uses different feature subsets for the prediction.  \n",
    "\n",
    "- A typical $m$ is $\\sqrt{p}$ where $p$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient boosting\n",
    "- Gradient boosting learns new models based on what was not learned before.  \n",
    "\n",
    "- Theory is strong deepâ€”outside of the scope of the courseâ€”but\n",
    "  - Boosting is the concept of combining multiple *weak learners* (e.g., barely better than chance) into a *wrong learner*.  \n",
    "  \n",
    "- Weak learners are strongly correlated because roughly speaking:\n",
    "  - Bagging aims at reducing variance.\n",
    "  - Boosting aims at reducing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "- 60% training, 20% validation, 10% testing.  \n",
    "\n",
    "- Regularized logistic regression: 63% AUC.  \n",
    "\n",
    "- Random forest: 84% AUC.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf18.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpreting fits\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-5\">\n",
    "      <ul>\n",
    "        <li>Logistic regression: easy</li>\n",
    "        <br>\n",
    "        <left><img src=\"./images/unit-09/unit-09-0_cscsrf19.png\" width=\"100%\" align=\"left\"></left>\n",
    "      </ul>        \n",
    "    </div>\n",
    "    <div class=\"col-7\">\n",
    "        <br>\n",
    "        <br>\n",
    "        <br>\n",
    "        <p>$$p(y \\mid X) = \\frac{1}{1 + \\exp(-(\\theta_0 + \\sum_{j>0} x_j\\theta_j))}$$</p>\n",
    "    </div>\n",
    "  </div>      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpreting fits (2)  \n",
    "\n",
    "- Random forest: compute average reduction in loss across trees.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-09/unit-09-0_cscsrf20.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take home messages\n",
    "- Sometimes we need non-linear decision boundaries.  \n",
    "\n",
    "- Decision trees give this boundaries but have high variance.  \n",
    "\n",
    "- We can reduce variance with bagging.  \n",
    "\n",
    "- Because of bootstrap, trees become correlated, reducing effectiveness of bagging.  \n",
    "\n",
    "- Random forest tries to decorrelate trees by subsampling features at each tree split."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
