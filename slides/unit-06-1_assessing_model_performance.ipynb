{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> <h1>Unit 6</h1> </center>\n",
    "<center> <h1>Assessing model accuracy</h1> </center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center> <h3>IST 718 – Big Data Analytics</h3> </center>\n",
    "<center> <h3>Daniel E. Acuna</h3> </center>\n",
    "<center> <h3>http://acuna.io</h3> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From previous unit\n",
    "- We took a **statistical approach to learning** which acknowledges our uncertainty and noise in the data science process.  \n",
    "\n",
    "- We defined a **model** of the data.  \n",
    "\n",
    "- We estimated the parameters using **training data**.  \n",
    "\n",
    "- We used the model to **predict** and **interpret** the results.  \n",
    "\n",
    "- We could use **supervised** or **unsupervised** learning to find relationships between variables.  \n",
    "\n",
    "- If variables are not quantitative, we used classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In this unit\n",
    "- Generalization performance\n",
    "- Estimating generalization performance: cross-validation\n",
    "- Bias-variance decomposition\n",
    "- Performance of classifiers: confusion matrix, ROC curve, AUC\n",
    "- Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/unit-06/unit-06-0_ama1.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/unit-06/unit-06-0_ama2.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization performance\n",
    "- Generalization is the performance of a learning method on independent **test data**.  \n",
    "\n",
    "- Generalization performance guides the choice of learning method or model.  \n",
    "- **Why don't we teach the *best method*?**\n",
    "  - *Because there is no free lunch in statistics*: David Wolpert, \"The Lack of A Priori Distinctions Between Learning Algorithms\", 1996.\n",
    "  <div class=\"blockquote2\">\n",
    "    No one method dominates all others over all possible data sets.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization performance (2)\n",
    "- The no free lunch theorem implies that we need to:  \n",
    "\n",
    "  1. Learn about the **particular dataset** we are working on (data science!)  \n",
    "  \n",
    "  2. **Select the best method** using generalization performance (data science!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: theory\n",
    "- We need to define a loss function:\n",
    "\n",
    "$$l(Y,\\hat{f}(X))$$  \n",
    "\n",
    "- For example, the Squared Error for regression:\n",
    "\n",
    "$$l(Y,\\hat{f}(X)) = (Y - \\hat{f}(X))^2$$  \n",
    "\n",
    "<center>(which is the same as the negative likelihood with Gaussian noise.)</center>  \n",
    "\n",
    "- Or zero-one loss for classification:\n",
    "\n",
    "$$l(Y,\\hat{f}(X)) = \\text{I}(Y \\neq \\hat{f}(X))$$  \n",
    "\n",
    "<center>where $\\text{I}(a,b)$ is 1 if $a = b$, and 0 otherwise.</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: test error and expected prediction error\n",
    "- Test error is the prediction error over an *independent* test sample:\n",
    "\n",
    "$$Err_T = E[l(Y,\\hat{f}(X)) \\mid T]$$  \n",
    "\n",
    "<center>where both $Y$ and $X$ are randomly sampled with a fixed training set $T$.</center>\n",
    "\n",
    "- A related quantity is the expected prediction error:\n",
    "\n",
    "$$Err = E[l(Y,\\hat{f}(X))] = E[Err_T]$$  \n",
    "\n",
    "<center>where everything is random including the training dataset.</center>\n",
    "\n",
    "- Most methods effectively estimate $Err$ instead of $Err_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: model comparison\n",
    "\n",
    "- Typically, we must compare several models\n",
    "- We split the data into **three datasets** and compute the following over a **testing dataset**\n",
    "$$Err_{V,T}=E[l(Y,\\hat{f}(X))\\mid V,T ]$$\n",
    "where $V$ is a **validation dataset**, and $T$ is a **training dataset**. \n",
    "- We further restrict the previous quantity to the best model on **validation** performance, and therefore we end up estimating\n",
    "$$Err = E[E_T[Err_{V^*,T}]]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating test error in practice: cross validation\n",
    "<br>\n",
    "\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "        <p>**Model select and assessment**:</p>\n",
    "          <ul>\n",
    "            <li>Often, models have differing degrees of complexity controlled by a parameter $\\alpha$ (e.g., $\\;\\hat{f}_\\alpha(X)$)</li>\n",
    "            <li>*Training split* is used to **fit** one model.</li>\n",
    "            <li>*Validation split* is used to select **complexity**.</li>\n",
    "            <li>*Test split* is used to estimate **expected test error**.</li>  \n",
    "          </ul>        \n",
    "    </div>\n",
    "  <div class=\"col-6\">\n",
    "    <ul>\n",
    "    <center><img src=\"./images/unit-06/unit-06-0_ama3.png\" width=\"100%\" align=\"center\"></center>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating test error in practice: cross validation (2)\n",
    "- The previous approach is known as **training, validation, and testing split**\n",
    "- If no alternative models are compared, there are only two splits and the method is known as training and testing\n",
    "- Typical data splits are 60%-30%-10% for training, validation, and testing\n",
    "- Or 80%-20% for training and testing splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating the expected test error: $k$-fold cross validation\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <li>The problem with the previous procedure is that we \"throw away\" the validation and test splits during training.</li>\n",
    "        <br>  \n",
    "        <li>$k$-fold cross validation (partially) fixes this by running cross validation multiple times.</li>\n",
    "      </ul>        \n",
    "    </div>\n",
    "  <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-06/unit-06-0_ama4.png\" width=\"100%\" align=\"center\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating the expected test error: training data splits\n",
    "- The **test split** should <u>**only**</u> be used at the end of the data science.\n",
    "- (Demo) We will compare two models in diabetes dataset: linear regression using BMI (M1) and linear regression using BMI and age (M2).  \n",
    "   \n",
    "  1. Model fit:  \n",
    "  M1 MSE (training) = 3723, **M2 MSE  (training) = 3680**\n",
    "  \n",
    "  2. Model selection:  \n",
    "  **M1 MSE (validation) = 4582**, M2 MSE (validation) = 4618\n",
    "    \n",
    "  3. Model assessment:  \n",
    "  **M1 MSE (test) = 3925**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating the expected test error: variable selection\n",
    "- This procedure can be generalized to answer the question:\n",
    "  - Which variables should be included in the model?  \n",
    "  \n",
    "- One of the simplest such procedures is the null-to-full model variable selection:\n",
    "<ol>\n",
    "    <li>For $k$ variables from 0 to $p$:</li>\n",
    "      <ol>\n",
    "        <li>Fit $p – k$ models which add one variable to the current model.</li>\n",
    "        <li>If none of the models has lower validation error than current model, then break.</li>\n",
    "        <li>Set the current model to the one with lowest validation error.</li>\n",
    "      </ol>           \n",
    "    <li>Return current model.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "- This procedure selects age, bmi, map, tc, ltg, and glu as the most important variables with estimated test error of **3324**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More on expected test error\n",
    "- The loss function used to cross validation **does not** necessarily match the loss function used during model fitting.\n",
    "- For example, you can fit models with gradient descent to minimize MSE (because it is easy and fast) but you might choose models based on the Mean Absolute Deviation (MAD)\n",
    "$$\\text{MAD} = \\frac{1}{n} \\sum_{i=1}^n | \\hat{y}_i - y |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition: Math\n",
    "- Mathematically:  \n",
    "\n",
    "$$\\begin{align}\n",
    "Err(x_0) &= E[(Y-\\hat{f}(x_0))^2\\mid x_0] \\\\\n",
    "Err(x_0) &= \\sigma_{\\epsilon}^2 + (E[\\hat{f}(x_0)]-f(x_0))^2 + E[\\hat{f}(x_0)-E[\\hat{f}(x_0)]]^2 \\\\\n",
    "Err(x_0) &= \\text{Irreducible error} + \\text{Bias}^2 + \\text{Variance}\n",
    "\\end{align}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition of test error\n",
    "- In general, more complex models have low bias and high variance.  \n",
    "\n",
    "- Vice versa, simple models have high bias and low variance.  \n",
    "\n",
    "- This is a **fundamental tradeoff**.  \n",
    "\n",
    "- **High variance** means that the estimation has high \"error bars.\"  \n",
    "\n",
    "- **High bias** means that the estimation will not change much even if more data is seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition: Nearest neighbor regression\n",
    "- Take the $k$ closest points and compute the average $y$ of those points\n",
    "$$\\hat{f}(x_0) = \\frac{1}{k}\\sum_{i=1}^k f(x_{\\text{nn}_i(x_0)})$$\n",
    "where $\\text{nn}_i(x)$ is the index of $i$-th closest neighbor to $x$\n",
    "- The Bias-variance decomposition then becomes\n",
    "$$Err(x_0) = \\sigma_\\epsilon^2 + [f(x_0) - \\frac{1}{k}\\sum_{i=1}^k f(x_{\\text{nn}_i(x_0)})]^2 + \\frac{\\sigma_\\epsilon^2}{k}$$\n",
    "where $\\frac{\\sigma_\\epsilon^2}{k}$ is the variance of $\\hat{f}$\n",
    "- Bias increases with $k$ and variance decreases with $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition: Linear regression\n",
    "- The argument is more complex: $\\hat{f}(x_0) = x_0^T b$ with vector $b$ having $p$ components\n",
    "- On average\n",
    "$$Err(x_0) \\approx \\sigma_\\epsilon^2 + \\text{Bias}^2 + p \\sigma_\\epsilon^2$$\n",
    "- Variance is proportional to the number of parameters and therefore Bias decreases with number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition: in practice\n",
    "- It is many times impossible to know the irreducible error, bias, and variance decomposition\n",
    "- There are several heuristics for trying to understand whether we need to increase the bias or the variance, or whether we have simply hit the irreducible error\n",
    "- We will want to **search over many models to find the one which minimizes bias and variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition (2)\n",
    "An example with the diabetes dataset:\n",
    "- We add a polynomial expansion to the variables:\n",
    "  - bmi, age, bmi*age, bmi$^2$, age$^2$, etc. \n",
    "  \n",
    "- We will have a new set of features of size:\n",
    "\n",
    "$$p_\\text{new} = 2p + p(p-1)/2$$\n",
    "\n",
    "- We run the following procedure:\n",
    "  Repeat many times:\n",
    "  1. Randomly split the data into training and testing\n",
    "  2. For $k=1$ to $p_\\text{new}$\n",
    "    1. Fit model with $k$ randomly selected features and predict.\n",
    "    2. Estimate training and testing MSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition (3)\n",
    "<br>\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama5.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias-Variance decomposition: the learning curve\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-6\">\n",
    "      <ul>\n",
    "        <li>Simple model doesn’t learn much after 50 examples.</li>\n",
    "        <li>Complex model keeps learning.</li>\n",
    "        <li>Simple model has better performance with small datasets.</li>\n",
    "        <li>Complex model has better performance with big datasets.</li>\n",
    "      </ul>        \n",
    "    </div>\n",
    "  <div class=\"col-6\">\n",
    "    <center><img src=\"./images/unit-06/unit-06-0_ama6.png\" width=\"100%\" align=\"center\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: classification\n",
    "- A typical method for measuring classification performance is *accuracy*:  \n",
    "\n",
    "    $$\\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{I}(y_i,\\hat{f}(x_i))$$\n",
    "<center>where $\\text{I}(a, b)$ is 1 if $a = b$, and 0 otherwise.</center>  \n",
    "\n",
    "\n",
    "- What might be the problem with using accuracy? (hint: What is the accuracy of an algorithm that predicts that every email received is not a spam?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: confusion matrix\n",
    "- **Activity**: Given one normal email (NE) and one spam email (SE), GMAIL can perform a non-spam classification (NC) and spam classification (SC). Where would you put all these cases in the following **matrix**?  \n",
    "<img src=\"images/unit-06/unit-06-0_ama7.png\" width=\"80%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: confusion matrix (2)\n",
    "- **Activity**: Given one normal email (NE) and one spam email (SE), GMAIL can perform a non-spam classification (NC) and spam classification (SC). Where would you put all these cases in the following **matrix**?  \n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama8.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: confusion matrix (3)\n",
    "- This matrix is called a **confusion matrix**: summary of true vs predicted cases\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama9.png\" width=\"80%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: Common statistics from confusion matrix\n",
    "- **Prevalence**: (TP+FN) / everything\n",
    "- **Precision**: TP / (TP + FP)\n",
    "- **Sensitivity**, **Recall**, or **True positive rate**: TP / true condition positive\n",
    "- **Specificity**: TN / true condition negative\n",
    "- **F1**: $\\frac{2*precision*recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve\n",
    "- In general, no single algorithm has the best sensitivity and specificity simultaneously.\n",
    "- Some algorithms offer a range of sensitivity/specificity points.  ROC curve displays this\n",
    "\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama10.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve (2)\n",
    "- We will classify as spam if classifier thinks with more than 50% probability.  \n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama11.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve (3)\n",
    "- **Less stringent, we will classify as spam if P(spam) > 0.1**\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama12.png\" width=\"100%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve (4)\n",
    "- We can put the two thresholds $\\theta = 0.5$ and $\\theta = 0.1$ in a plot.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama13.png\" width=\"50%\" align=\"center\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve  (5)\n",
    "- We can do this for every threshold value and plot the result as a curve.  \n",
    "<br>\n",
    "<center><img src=\"./images/unit-06/unit-06-0_ama14.png\" width=\"50%\" align=\"center\"></center>\n",
    "<br>\n",
    "<u>**Activity**</u>: can you guess the curve of a random classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring generalization performance: ROC curve (6)\n",
    "- We can do this for every threshold value and plot the result as follows:  \n",
    "\n",
    "<br>\n",
    "<div class=\"container2\">\n",
    "  <div class=\"row2\">\n",
    "    <div class=\"col-8\">\n",
    "      <center><img src=\"./images/unit-06/unit-06-0_ama15_2.png\" width=\"50%\" align=\"center\"></center>\n",
    "    </div>\n",
    "  <div class=\"col-4\">\n",
    "    <br>  \n",
    "    <p>A measure that combines all thresholds is the **Area Under the ROC Curve (AUC)**</p>\n",
    "</div>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Area under the ROC curve (AUC)\n",
    "\n",
    "- It is hard to interpret but:\n",
    "    - It can be thought as how good is the model to rank the probabilities with the real labels\n",
    "- The **AUC** is 1/2 if the classifier is *random* or *constant*\n",
    "- **Activity: show that the AUC is in fact 1/2 for a random predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AUC for random prediction\n",
    "\n",
    "- AP, AN, PP, PN mean actual positive, actual negative, predicted positive, and predictived negative, respectively\n",
    "- There are $n$ cases in total\n",
    "\n",
    "| .  | PP|PN |\n",
    "|---|---|---|\n",
    "| AP| TP|FN |\n",
    "| AN| FP|TN |\n",
    "\n",
    "- Cases are gonna randomly fall on PP and PN. \n",
    "- For a given threshold $\\theta$, FP = $(1-\\theta)$AN and TN=$\\theta$AN, and similarly TP=$(1-\\theta)$AP and FN=$\\theta$AP\n",
    "- FPR = $(1-\\theta)$AN/($(1-\\theta)$AN + $\\theta$AN) = $1-\\theta$\n",
    "- Therefore, AUC = $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AUC for constant prediction\n",
    "- As we move the threshold there will be a change between all points being in PP to PN, or viceverse.\n",
    "- There two points will have FPR=TPR=0 and FPR=TPR=1 respectively\n",
    "- By interpolation, AUC = $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bayesian classifier\n",
    "- Many values from confusion matrix are misleading because we are not using *prevalence rates*.  \n",
    "\n",
    "- Maximum likelihood estimators estimate the most likely classification of the data given a hypothesis (actual class) without consideration of the *prevalance rate*\n",
    "\n",
    "- The optimal decision however is achieved by Bayes' theorem\n",
    "\n",
    "$$p(\\text{hypothesis} \\mid \\text{data}) = \\frac{p(\\text{data} \\mid \\text{hypothesis})\\;p(\\text{hypothesis})}{p(\\text{data})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bayesian classifier (2)\n",
    "- Sometimes classification performance in terms of metrics that do not consider prevalance.\n",
    "- For example, a classifier for cancer (C) given positive results (R) or might be reported with:\n",
    "  - Specificity: 95%\n",
    "  - Sensitivity: 60%  \n",
    "- But cancer prevalance is low (0.2%)\n",
    "$$p(\\text{C}\\mid \\text{R}) = \\frac{p(\\text{R}\\mid\\text{C})p(\\text{C})}{p(\\text{R})}$$\n",
    "with\n",
    "$$p(\\text{R}) = p(\\text{R}\\mid\\text{C})p(\\text{C}) + p(\\text{R}\\mid\\neg\\text{C})p(\\neg\\text{C})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Take home message\n",
    "- Always keep the testing dataset in vault until the end of your analysis.  \n",
    "\n",
    "- Use model selection to choose from models of different complexity.  \n",
    "\n",
    "- Stick to a loss function throughout your analysis.  \n",
    "\n",
    "- For classification problems, think carefully about the requirements of your problem.  \n",
    "\n",
    "- Be careful about the prevalence values."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
